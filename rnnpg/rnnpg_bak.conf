alpha = 0.001625    	 # -- learning rate
alphaDiv = 2    	 # -- learning rate decay
beta = 0.0000001     # -- regulation weight
hidden = 200      # -- hidden layer size
class = 82      # -- class size
iter = 1000      # -- max iteration
fixFirst = false  # -- true means fix the first layer of the sentence model
randInit = false  # -- true means randomly initialize sentence model word embedding; false means using word2vec to initialize the layer
randSeed = 1
stableAC = 0.1
flushOption = 2  # -- 1, flush the network every sentence; 2, flush the network every poem; 3, never flush (only flush at the beginning of training)
minImp = 1.0001
consynMin = -10
consynMax = 10
historyStableAC = 0.01
consynOffset = 0
direct = false
saveModel = 2
modelF = models/200
trainF = /afs/inf.ed.ac.uk/user/s12/s1270921/Desktop/programming/Run/RNNPG/Data/quatrain/quatrain-train-valid-test-rare/quatrain.train.uniq.rare
validF = /afs/inf.ed.ac.uk/user/s12/s1270921/Desktop/programming/Run/RNNPG/Data/quatrain/quatrain-train-valid-test-rare/quatrain.valid
testF  = /afs/inf.ed.ac.uk/user/s12/s1270921/Desktop/programming/Run/RNNPG/Data/quatrain/quatrain-train-valid-test-rare/quatrain.test
embedF = /afs/inf.ed.ac.uk/user/s12/s1270921/Desktop/programming/Run/RNNPG/Models/word2vec/poem.vectors.rare1.200.bin
vocabClassF = /afs/inf.ed.ac.uk/user/s12/s1270921/Desktop/programming/Run/RNNPG/Models/vocab_class/output/vocab.class.82.txt
